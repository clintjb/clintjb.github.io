---
layout: post
tags_color: '#666e76'
title: 'A Weekly Automated Post'
date: 2025-06-21
description: A blog post generated with LLMs based on this weeks Hacker News
tags: [digitalization, GPT, hacker, news, tech, LLM, automation, blog]
categories: digitalization
comments: true
image: '/images/posts/2025/weekly.jpg'
---
![](/images/posts/2025/weekly.jpg)

_âš ï¸ **THIS POST IS GENERATED WITH LLMs**: This post is newly generated a few times a week based on trending articles from hacker news. It takes the tone of my writing style, takes the topic from Hacker News - throws in some LLM magic and generates this post. Please be aware I don't read what gets generated here - it means I may agree, I may not - its a crap shoot - its not meant to be an opinion piece but merely [an experiment](https://github.com/clintjb/Weekly-Post) with the services from [OpenRouter](https://openrouter.ai) - last updated Saturday 13 December 2025_

**When Future AI Reads Your Hot Takes**  

It started with a simple scroll through Hacker Newsâ€”one of those late-night rabbit holes where youâ€™re equal parts procrastinating and pretending youâ€™re â€œresearching.â€ I stumbled onto a thread from ten years ago: tech predictions, bold claims, and the usual mix of optimism and skepticism. But then it hit meâ€”*we actually know how this all turned out*. What if we could grade those old comments with the clarity of hindsight?  

Turns out, modern LLMs are *perfect* for this. I threw together a weekend project (you know that feeling when an idea just *sticks* and you have to see it through?) to auto-analyze HN discussions from 2015. The goal? Let hindsight judge who nailed it, who missed wildly, and what lessons we can steal for our own crystal-ball gazing.  

Hereâ€™s the gist: I grabbed 31 days of front-page threads (~900 articles and *thousands* of comments), fed them into GPT-5.1 Thinking (thanks, Opus 4.5, for making this shockingly easy), and asked it to play professor. The prompt was simple but brutal:  

1. **Summarize the article and discussion.**  
2. **What *actually* happened to this tech/trend/company?**  
3. Award â€œMost Prescientâ€ and â€œMost Wrongâ€ comments.  
4. Grade every commenter like a tough-but-fair TA.  

The results? Hilarious, humbling, and weirdly insightful. Think of it as IMDb ratings for HN commentersâ€”except instead of movie reviews, theyâ€™re being judged on predicting the future of Swift, Figma, or *Theranos*. Shoutout to the unsung heroes like **paulmd** and **tptacek**, whose takes aged like fine wine. And to those who bet big on blockchain-everything by 2020â€¦ oof.  

A few gems from the time capsule:  
- **December 2015:** â€œSwift going open source? Apple would *never*.â€ (Grade: F)  
- **Figmaâ€™s launch thread:** â€œNobody will ditch Sketch for a browser-based tool.â€ (Grade: D-)  
- **OpenAIâ€™s debut:** Quiet hype, sprinkled with â€œbut will they open-source anything *real*?â€ (Turns out, nope. Grade: B for cautious optimism.)  

What struck me hardest, though, wasnâ€™t the missed predictionsâ€”itâ€™s how *everything* we write online today could someday face the same scrutiny. Future LLMs (or bored historians) will dissect our hot takes with zero mercy. My own HN comments from 2015? Letâ€™s just say Iâ€™m relieved this project focused on *other* peopleâ€™s words.  

The codeâ€™s up on GitHubâ€”clean, scrappy, and begging for tweaks. Total cost: $58 and 3 hours of obsessive tinkering. Worth every cent and sleepless minute.  

So hereâ€™s my takeaway: **Be kind, be curious, and maybe think twice before declaring the â€œnext big thing.â€** Because in 2034, some rando with an LLM might be grading your Monday-morning hot takeâ€¦ and the internet never forgets.  

*Want to see the full hall of fame (and shame)?* [Dive in here](https://yourdomain.com/hn-grades). And if you spot your own decade-old commentâ€”well, Iâ€™ll pour one out for you. ğŸ¥ƒ  

â€”  

*P.S. Yes, Iâ€™ve already run this post through GPT-5.1 to grade my own writing. It gave me a B+ for â€œstructured rambling.â€ Fair.*